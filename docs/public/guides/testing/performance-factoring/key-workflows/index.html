<!doctype html><html lang=en><head><script integrity=sha384-pb++s6uBRKaQv+iAXpgA/H3IlpLZdO14tTwuCI7uXmz4aaZdByoCcM+6BhynMq/1 src=https://docs.nosqlbench.io/js/theme.min.js></script><link href="https://docs.nosqlbench.io/abridge.css?h=17d1d14ae374495df55a" rel=stylesheet><meta charset=utf-8><meta content="ie=edge" http-equiv=x-ua-compatible><meta content="width=device-width,initial-scale=1" name=viewport><meta content=https://docs.nosqlbench.io name=base><meta content=True name=HandheldFriendly><meta content=yes name=mobile-web-app-capable><meta content=yes name=apple-mobile-web-app-capable><meta content=default name=apple-mobile-web-app-status-bar-style><link href=https://docs.nosqlbench.io/favicon.svg rel=icon type=image/svg+xml><link title="NoSQLBench Documentation Atom Feed" href=https://docs.nosqlbench.io/atom.xml rel=alternate type=application/atom+xml><meta content="index, follow" name=robots><meta content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1" name=googlebot><meta content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1" name=bingbot><title>Key Workflows | NoSQLBench Documentation</title><meta content="NoSQLBench Team" name=author><meta content="NoSQLBench Documentation" name=copyright><meta content="Common performance testing workflows and methodologies" name=description><link href=https://docs.nosqlbench.io/guides/testing/performance-factoring/key-workflows/ rel=canonical><meta content=https://docs.nosqlbench.io/guides/testing/performance-factoring/key-workflows/ property=og:url><meta content=https://docs.nosqlbench.io/guides/testing/performance-factoring/key-workflows/ name=twitter:url><meta content="Common performance testing workflows and methodologies" property=og:description><meta content="Common performance testing workflows and methodologies" name=twitter:description><meta content="Key Workflows | NoSQLBench Documentation" property=og:title><meta content="Key Workflows | NoSQLBench Documentation" name=twitter:title><meta content=summary name=twitter:card><meta content="NoSQLBench Documentation" property=og:site_name><meta content=en_US property=og:locale><meta content=website property=og:type><meta property=og:updated_time><script src="https://docs.nosqlbench.io/js/theme_button.js?h=12a31e87fcbe7a55e953" defer integrity=sha384-UBxLGgFVZtEgNzOU3WqKoFT1oILftGjShFO4WIs1dO/jK7SNyz7BMYCYt5PffTxe></script><script src="https://docs.nosqlbench.io/js/email.js?h=ee3dd742e11c5014fdef" defer integrity=sha384-y0LUfjslyVLYZvCOyhELpvKivlXAq27Vnbed54glXoe/pZSYS/Pfqghp/sQt1xcV></script><script src="https://docs.nosqlbench.io/js/codecopy.js?h=0a54f99e682443925d3a" defer integrity=sha384-qssVvPpQgISh4PFkSBGTgXEMzuFO8Hat5m1MsdkpannoByWSTpLfBe4iEVl3+RiP></script><script src="https://docs.nosqlbench.io/js/elasticlunr.min.js?h=4f648b9e42abdef9c436" defer integrity=sha384-Q8viz7rndi8MSNDtItDgVWNSrCumjepopRMlz2nElkWPRXDQAcfiZGJbrCJVXdtw></script><script src="https://docs.nosqlbench.io/search_index.en.js?h=e2df0fd710d0c807a7aa" defer integrity=sha384-nyOftseEtxo2Pc23RFR1hxsCZHtUr0xOCP7JVJ8oGIs+cnDq6GZkzCq2hWZ2cJIh></script><script src="https://docs.nosqlbench.io/js/searchjava.js?h=36b818b07e8b2b318967" defer integrity=sha384-i6K7Uy98ZLnOwn6no8sDUTeBitDmiLHOKxkcJYbjgu6vQ3hdyMQPzmeuX04BRs8J></script><noscript><link href=https://docs.nosqlbench.io/nojs.css rel=stylesheet></noscript><body><header><nav><div><big><a title="NoSQLBench Documentation" href=https://docs.nosqlbench.io></a></big></div><div><div><ul><li><a href=https://docs.nosqlbench.io/tutorials/> Tutorials </a><li><a href=https://docs.nosqlbench.io/guides/> Guides </a><li><a href=https://docs.nosqlbench.io/reference/> Reference </a><li><a href=https://docs.nosqlbench.io/explanations/> Explanations </a><li><a href=https://docs.nosqlbench.io/development/> Development </a><li><a href=https://github.com/nosqlbench/nosqlbench target=_blank> GitHub </a><li><i class="js svgs adjust" id=mode type=reset></i></ul></div><div><div><form autocomplete=off class=js id=searchbox name=goSearch><div class=searchd><input id=searchinput placeholder=Search title=Search><button class="svgs svgm search" title=Search></button></div><div class=results><div id=suggestions></div></div></form></div></div></div></nav></header><main><article><h1><a href=https://docs.nosqlbench.io/guides/testing/performance-factoring/key-workflows/>Key Workflows</a></h1><p>There are a few workflows which we see routinely in performance testing. These workflows capture, or at least describe the steps in a complete analysis method, and are the basis for extant analysis method scripts in NoSQLBench. This section describes some of these, the steps involved, and the purposes of each step.<p>On the surface, many of these techniques may appear to be pretty basic. Descriptions like “find the highest throughput” are an egregious misnomer for what is required to do this type of work accurately and precisely. This is important for everyone to understand who depends on the results of performance testing. Over-simplifying or rushing past these details can effectively invalidate the results to the point of being less useful than guesswork, particularly if the stakeholders presume a degree of methodical approach or empiricism which is not present.<h1 id=system-preparation>System Preparation</h1><ol><li>Deploy Testing apparatus, infrastructure, and target systems.<li>Document deployment details, including steps needed to redeploy the same type of system with the same provisioning levels, settings, topology, etc.</ol><p>Once a system is deployed for testing, all the essential details of the system which define the test scenario should be captured.<p>Empirically, this is every detail of the system, but practically speaking this is often not possible or convenient for these details to be capture. A key strategy is to use well-defined reference points, like a system image, default configuration, hardware profile, etc., and then document only deltas from this initial state. This also emphasizes the value of testing system as shipped or configured by default, since this is also a meaningful reference point for any other analysis.<h1 id=measure-saturating-throughput>Measure Saturating Throughput</h1><p>To balance and inform how performance data is interpreted, it is crucial to consider the throughput and response (AKA latency) in contrast and in combination. In order to do this well, it is essential to determine the maximum rate at which a system can process requests for the workload under study. This is what <em>measuring saturating throughput</em> is all about.<p>The saturating throughput for a composed system may be limited by any component, including clients, infrastructure, servers or endpoints, proxy layers, storage subsystems, etc. By definition, it <em>is</em> dependent on each and every part of the composed system. However, it is almost always more dependent on one component than others. This is often called a <em>bottleneck.</em> When a given component is disproportionately utilized over others, the system is may not be considered well-balanced. When this component hits full saturation, limiting the throughput of the composed system, then it is called a bottleneck. It is not always easy to define what constitutes a meaningful bottleneck when resource utilization is relatively even. Over-tuning for full saturation can be counterproductive.<p>It is essential to understand the general state of balance of a system at saturation. If optimizing for throughput, then key metrics should include any skews in workload distribution over nodes, resources, or services. As well, within vertical resource profiles, such as within a node, serious imbalances may invalidate the purposes of a test. This all depends on the specific reason for running the test.<p>Yet, it is possible, in a well-balanced system, that many components are <em>highly</em> saturated together. In many cases, this a desired state of balance. In a well-balanced system, appreciable speed-ups require creating more headroom (scaling up capacity) in all the components or subsystems. Once this is achieved in vertical resource profiles, simpler scale-out strategies become available, wherein you know each unit of capacity is representative of a unit of consumption for the given workload.<p>In practice, valid results are only possible when the target being tested is the limiting factor. Further, as the testing apparatus sees higher utilization (client-side or infrastructure) , the fidelity of results decreases. The relationship between client saturation and measurement accuracy is not well-defined, but is nearly always a non-linear relationship. For example running the client system at 60% utilization will certainly increase the measured latency of the composed system over the same test rate on a client which would only be 40% utilized. It is important to remember that whey you are running a test, there is no way to <em>only test the server</em>. However, you can shift the measurements to be more descriptive of the test target by ensuring that the whole system is over-provisioned in the testing apparatus as a rule.<h2 id=steps>Steps</h2><ol><li>Prepare target system, infrastructure, and client systems.<li>Instrument client system for basic metrics capture, including throughput and discrete latency histograms. (Avoid time-decaying or other leavening effects.) <ol><li>(Advanced) Instrument each key subsystem, messaging layer, system boundary, and resource pool in the entire composed system.<li>(Advanced) Baseline key subsystems for capacity using automated benchmarking tools.<li>(Advanced) Verify or record performance congruity and coherence across tested systems.</ol><li>Configure workload at sufficient concurrency. Minimum concurrency should keep all messaging paths primed at all times (transport, buffering requests, processing elements), with minimal over-commit. A good rule of thumb is to set concurrency to 2X estimated operational parallelism.<li>Method 1 - Run the workload at the full capacity of the client, adjusting the concurrency to find the local maxima in throughput. Adjust settings as need to optimize throughput until further improvement is minimal.<li>Method 2 - Run the workload with a rate limiter on the client side as the limiting factor. Adjust the rate limit to find the local maxima in throughput. Adjust setting as needed to optimize throughput until further improvement is minimal.<li>Method 3 - Use an automated and iterative analysis method like findmax in order to streamline the testing time, and codify the analysis method for reproducible and specific results.<li>Method 4 - Use an automated and iterative analysis method like optimo in order to genearlize over an n-dimensional parameter space which includes concurrency and other dynamic settings.<li>Record the <strong>result: maximum throughput and the settings required to achieve it</strong>.<li>(Advanced) Record the response curve of the system across key throughput stages.</ol><h1 id=measure-ideal-latency>Measure Ideal Latency</h1><p>Determine the latency of an operation under the best possible circumstances, i.e. all JIT, cache-warming is done, indexes and compaction state are optimal, and so on.</article></main><footer><div class=c><nav class=tpad><div></div></nav><p>© <span id=year>2025</span> NoSQLBench Documentation<p>Powered by <a href=https://www.getzola.org/ target=_blank>Zola</a> & <a href=https://github.com/jieiku/abridge/ target=_blank>Abridge</a></div></footer><span class=topout> <span class=topleft> </span><a title="Back to Top" class=top href=#><i class="svgs svgh angu"></i></a> </span>