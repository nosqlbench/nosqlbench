<!doctype html><html lang=en><head><script integrity=sha384-pb++s6uBRKaQv+iAXpgA/H3IlpLZdO14tTwuCI7uXmz4aaZdByoCcM+6BhynMq/1 src=https://docs.nosqlbench.io/js/theme.min.js></script><link href="https://docs.nosqlbench.io/abridge.css?h=17d1d14ae374495df55a" rel=stylesheet><meta charset=utf-8><meta content="ie=edge" http-equiv=x-ua-compatible><meta content="width=device-width,initial-scale=1" name=viewport><meta content=https://docs.nosqlbench.io name=base><meta content=True name=HandheldFriendly><meta content=yes name=mobile-web-app-capable><meta content=yes name=apple-mobile-web-app-capable><meta content=default name=apple-mobile-web-app-status-bar-style><link href=https://docs.nosqlbench.io/favicon.svg rel=icon type=image/svg+xml><link title="NoSQLBench Documentation Atom Feed" href=https://docs.nosqlbench.io/atom.xml rel=alternate type=application/atom+xml><meta content="index, follow" name=robots><meta content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1" name=googlebot><meta content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1" name=bingbot><title>Labeling Controls | NoSQLBench Documentation</title><meta content="NoSQLBench Team" name=author><meta content="NoSQLBench Documentation" name=copyright><meta content="Filtering and validating labels for consistent test data" name=description><link href=https://docs.nosqlbench.io/guides/workload-design/labeling/labeling-controls/ rel=canonical><meta content=https://docs.nosqlbench.io/guides/workload-design/labeling/labeling-controls/ property=og:url><meta content=https://docs.nosqlbench.io/guides/workload-design/labeling/labeling-controls/ name=twitter:url><meta content="Filtering and validating labels for consistent test data" property=og:description><meta content="Filtering and validating labels for consistent test data" name=twitter:description><meta content="Labeling Controls | NoSQLBench Documentation" property=og:title><meta content="Labeling Controls | NoSQLBench Documentation" name=twitter:title><meta content=summary name=twitter:card><meta content="NoSQLBench Documentation" property=og:site_name><meta content=en_US property=og:locale><meta content=website property=og:type><meta property=og:updated_time><script src="https://docs.nosqlbench.io/js/theme_button.js?h=12a31e87fcbe7a55e953" defer integrity=sha384-UBxLGgFVZtEgNzOU3WqKoFT1oILftGjShFO4WIs1dO/jK7SNyz7BMYCYt5PffTxe></script><script src="https://docs.nosqlbench.io/js/email.js?h=ee3dd742e11c5014fdef" defer integrity=sha384-y0LUfjslyVLYZvCOyhELpvKivlXAq27Vnbed54glXoe/pZSYS/Pfqghp/sQt1xcV></script><script src="https://docs.nosqlbench.io/js/codecopy.js?h=0a54f99e682443925d3a" defer integrity=sha384-qssVvPpQgISh4PFkSBGTgXEMzuFO8Hat5m1MsdkpannoByWSTpLfBe4iEVl3+RiP></script><script src="https://docs.nosqlbench.io/js/elasticlunr.min.js?h=4f648b9e42abdef9c436" defer integrity=sha384-Q8viz7rndi8MSNDtItDgVWNSrCumjepopRMlz2nElkWPRXDQAcfiZGJbrCJVXdtw></script><script src="https://docs.nosqlbench.io/search_index.en.js?h=e2df0fd710d0c807a7aa" defer integrity=sha384-nyOftseEtxo2Pc23RFR1hxsCZHtUr0xOCP7JVJ8oGIs+cnDq6GZkzCq2hWZ2cJIh></script><script src="https://docs.nosqlbench.io/js/searchjava.js?h=36b818b07e8b2b318967" defer integrity=sha384-i6K7Uy98ZLnOwn6no8sDUTeBitDmiLHOKxkcJYbjgu6vQ3hdyMQPzmeuX04BRs8J></script><noscript><link href=https://docs.nosqlbench.io/nojs.css rel=stylesheet></noscript><body><header><nav><div><big><a title="NoSQLBench Documentation" href=https://docs.nosqlbench.io></a></big></div><div><div><ul><li><a href=https://docs.nosqlbench.io/tutorials/> Tutorials </a><li><a href=https://docs.nosqlbench.io/guides/> Guides </a><li><a href=https://docs.nosqlbench.io/reference/> Reference </a><li><a href=https://docs.nosqlbench.io/explanations/> Explanations </a><li><a href=https://docs.nosqlbench.io/development/> Development </a><li><a href=https://github.com/nosqlbench/nosqlbench target=_blank> GitHub </a><li><i class="js svgs adjust" id=mode type=reset></i></ul></div><div><div><form autocomplete=off class=js id=searchbox name=goSearch><div class=searchd><input id=searchinput placeholder=Search title=Search><button class="svgs svgm search" title=Search></button></div><div class=results><div id=suggestions></div></div></form></div></div></div></nav></header><main><article><h1><a href=https://docs.nosqlbench.io/guides/workload-design/labeling/labeling-controls/>Labeling Controls</a></h1><p>üëâ NOTE: The labeling control features are new within NoSQLBench and are subject to changes.<h2 id=labeling-standards>Labeling Standards</h2><p>In order to make sure that your metrics and annotations have the necessary labels so that the data is traceable, identifiable, and contextual, you may want to set some label validation rules. These rules simply say which label names are required and which are not allowed. If you have a shared system of record for your metrics with other engineers or teams, then some discipline about how these metrics are reported is always required, but not often adhered to, even if there is a shared idea of what the metrics should look like. The net effect is that some of your most useful data gets lost among and indistinguishable from a trove of other metrics data which is extraneous, or worse, your data has this effect on others‚Äô.<p>It is recommended that you have a labeling standard, and that is is somewhere that is easy to share, verify, and enforce. For lack of this, NoSQLBench provides a client-side mechanism to make this possible in practice, although it is opt-in and not enforced from any consuming system. It is expected that NoSQLBench will support loading any provided labeling standards from consuming system automatically in the future.<p>Some specific examples for why this is important:<ul><li>You want to look the results of a test which were run 6 months ago, there are 37 other sets of metrics data from the same time frame. Either your data is labeled so you can distinguish the studies, or you‚Äôre just out of luck. You could have simply required a single label with a unique identifier in your standard to avoid this.<li>You want to aggregate results across a set of nodes. Either your data is distinguished by node, or you have bad data. You could simply require a node label to be provided, within the context of your other labels. The validity of your aggregating metrics depends directly on this being stable and distinctly addressable.</ul><h3 id=filtering-and-validation>Filtering and Validation</h3><p>Example label set:<pre style=color:#c0c5ce;background-color:#2b303b><code><span>alpha=one,beta=two,gamma=three,delta=four
</span></code></pre><h4 id=labeling-specifications>Labeling Specifications</h4><p>There are two ways to ensure that labels you send are valid:<ol><li>Adjust the provided labels to conform to the standard before sending.<li>Validate that the labels provided already conform to the standard before sending.</ol><p>These two are distinct methods which are complimentary, and thus NB does both. This is because there are label sets which can be aligned to a standard simply by filtering, and there are other cases where this is not possible due to missing data.<h3 id=label-specs>Label Specs</h3><p>These two method are combined in practice into one configuration parameter (for now at least), since the expression of validation and pre-filtering for that validation is effectively one and the same. This is known as the <code>labelspec</code>, and it is simply a one-line pattern which specifies which labels are required, which labels are disallowed, and so on.<h3 id=label-filtering>Label Filtering</h3><ul><li>Included labels are indicated as <code>+&LTlabelname></code> or simply <code>&LTlabelname></code>.<li>Excluded labels are indicated as <code>-&LTlabelname></code>.<li>The labelname can be any valid regular expression, and is always treated as such. Bare words are simply literal expressions.<li>If a provided label set includes label names which are not indicated as either, then they are Included by default.</ul><p>So why would you have a bare label name in lieu of <code>+</code> or vice-versa if they mean the same thing? The reason is that the labelname filter operates as a tri-state filter, going from left to right over the labelspec. You can include, or exclude any label name, and you can also use <em>wildcard</em> patterns simply by using regular expressions. Thus, a spec of <code>lioness,-lion.*</code> indicates that you want to allow <code>lioness</code> labels but disallow any other labels which start with <code>lion</code>. However, this does not say that you <em>require</em> a <code>lioness</code> label, which looks like this <code>+lioness,-lion.*</code>. In both cases, any label which is not mentioned will pass through the filter unchanged. If you wanted to reject all others, you can always end the labelspec with a <code>-.*</code>, such as <code>+lioness, -lion.*,-.*</code>, which is also duplicitous and can simply be reduced to <code>+lioness,-.*</code>.<h4 id=label-filtering-effects>Label Filtering Effects</h4><p>The label filter is applied at the point of label usage on outbound signals. Metrics and Annotations both have the filter applied. The underlying label sources are not affected. When a label set is passed through the label filter, some fields may be removed. Otherwise label filtering is entirely passive. The order of labels is preserved if they came from an ordered map.<h3 id=label-validation>Label Validation</h3><p>Label validation occurs on a label set immediately after filtering. The specification is identical, although it weighs a little more heavily in terms of side-effects for validation.<ul><li>Required label names are indicated with <code>+&LTlabelname></code><li>Disallowed label names are indicated with <code>-&LTlabelname></code><li>Bare words are disregarded by validation.<li>Patterns are still applied with the expected behavior.</ul><h4 id=label-validation-effects>Label Validation Effects</h4><p>Label validation occurs immediately after label filtering, and will throw an error in your test, with an error that indicates why the label set could not be validated.<p>This is because invalid labels will cause pain downstream. In most cases, this will manifest early in a test so that you don‚Äôt have any wasted bake time. It is recommended that you always run a full end-to-end functional validation of all your test workflows before running the more intensive versions. This helps you shake out any issues with late-validation, such as a bad label set in a later stage of testing.<h2 id=command-line-usage>Command Line Usage</h2><p>You can set labeling specifications on the command line:<pre class=language-shell data-lang=shell style=color:#c0c5ce;background-color:#2b303b><code class=language-shell data-lang=shell><span># Set both metrics and annotation label specs
</span><span>nb5 --labelspec "+instance,+session,+node" ...
</span><span>
</span><span># Set different specs for metrics vs annotations
</span><span>nb5 --metrics-labelspec "+instance,+session" \
</span><span>    --annotate-labelspec "+instance,+session,+region" ...
</span></code></pre><p>See the <a href=../../../reference/cli/options.md#labeling-options>CLI options reference</a> for complete details.<h2 id=best-practices>Best Practices</h2><ol><li><strong>Establish Standards Early</strong> - Define your labeling standards before running production tests<li><strong>Use Named Scenarios</strong> - They provide automatic, consistent labeling<li><strong>Require Key Labels</strong> - Always require at minimum: instance, session, node<li><strong>Test Validation</strong> - Run short validation tests before long performance runs<li><strong>Document Your Standards</strong> - Share labelspecs with your team</ol><h2 id=future-enhancements>Future Enhancements</h2><p>Conditional validation is planned to support more sophisticated rules, such as:<pre style=color:#c0c5ce;background-color:#2b303b><code><span>+appname,+instance:\w+,+session,+node,activity->dataset,activity->dimensions
</span></code></pre><p>This would express rules like ‚Äúif activity label is present, then dataset must also be present.‚Äù</article></main><footer><div class=c><nav class=tpad><div></div></nav><p>¬© <span id=year>2025</span> NoSQLBench Documentation<p>Powered by <a href=https://www.getzola.org/ target=_blank>Zola</a> & <a href=https://github.com/jieiku/abridge/ target=_blank>Abridge</a></div></footer><span class=topout> <span class=topleft> </span><a title="Back to Top" class=top href=#><i class="svgs svgh angu"></i></a> </span>