<!doctype html><html lang=en><head><script integrity=sha384-pb++s6uBRKaQv+iAXpgA/H3IlpLZdO14tTwuCI7uXmz4aaZdByoCcM+6BhynMq/1 src=https://docs.nosqlbench.io/js/theme.min.js></script><link href="https://docs.nosqlbench.io/abridge.css?h=17d1d14ae374495df55a" rel=stylesheet><meta charset=utf-8><meta content="ie=edge" http-equiv=x-ua-compatible><meta content="width=device-width,initial-scale=1" name=viewport><meta content=https://docs.nosqlbench.io name=base><meta content=True name=HandheldFriendly><meta content=yes name=mobile-web-app-capable><meta content=yes name=apple-mobile-web-app-capable><meta content=default name=apple-mobile-web-app-status-bar-style><link href=https://docs.nosqlbench.io/favicon.svg rel=icon type=image/svg+xml><link title="NoSQLBench Documentation Atom Feed" href=https://docs.nosqlbench.io/atom.xml rel=alternate type=application/atom+xml><meta content="index, follow" name=robots><meta content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1" name=googlebot><meta content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1" name=bingbot><title>DNN_angular1_neighbors | NoSQLBench Documentation</title><meta content="NoSQLBench Team" name=author><meta content="NoSQLBench Documentation" name=copyright><meta content="Auto-generated reference documentation for DNN_angular1_neighbors" name=description><link href=https://docs.nosqlbench.io/reference/bindings/funcref-experimental/ rel=canonical><meta content=https://docs.nosqlbench.io/reference/bindings/funcref-experimental/ property=og:url><meta content=https://docs.nosqlbench.io/reference/bindings/funcref-experimental/ name=twitter:url><meta content="Auto-generated reference documentation for DNN_angular1_neighbors" property=og:description><meta content="Auto-generated reference documentation for DNN_angular1_neighbors" name=twitter:description><meta content="DNN_angular1_neighbors | NoSQLBench Documentation" property=og:title><meta content="DNN_angular1_neighbors | NoSQLBench Documentation" name=twitter:title><meta content=summary name=twitter:card><meta content="NoSQLBench Documentation" property=og:site_name><meta content=en_US property=og:locale><meta content=website property=og:type><meta content=2025-11-13 property=og:updated_time><script src="https://docs.nosqlbench.io/js/theme_button.js?h=12a31e87fcbe7a55e953" defer integrity=sha384-UBxLGgFVZtEgNzOU3WqKoFT1oILftGjShFO4WIs1dO/jK7SNyz7BMYCYt5PffTxe></script><script src="https://docs.nosqlbench.io/js/email.js?h=ee3dd742e11c5014fdef" defer integrity=sha384-y0LUfjslyVLYZvCOyhELpvKivlXAq27Vnbed54glXoe/pZSYS/Pfqghp/sQt1xcV></script><script src="https://docs.nosqlbench.io/js/codecopy.js?h=0a54f99e682443925d3a" defer integrity=sha384-qssVvPpQgISh4PFkSBGTgXEMzuFO8Hat5m1MsdkpannoByWSTpLfBe4iEVl3+RiP></script><script src="https://docs.nosqlbench.io/js/elasticlunr.min.js?h=4f648b9e42abdef9c436" defer integrity=sha384-Q8viz7rndi8MSNDtItDgVWNSrCumjepopRMlz2nElkWPRXDQAcfiZGJbrCJVXdtw></script><script src="https://docs.nosqlbench.io/search_index.en.js?h=e2df0fd710d0c807a7aa" defer integrity=sha384-nyOftseEtxo2Pc23RFR1hxsCZHtUr0xOCP7JVJ8oGIs+cnDq6GZkzCq2hWZ2cJIh></script><script src="https://docs.nosqlbench.io/js/searchjava.js?h=36b818b07e8b2b318967" defer integrity=sha384-i6K7Uy98ZLnOwn6no8sDUTeBitDmiLHOKxkcJYbjgu6vQ3hdyMQPzmeuX04BRs8J></script><noscript><link href=https://docs.nosqlbench.io/nojs.css rel=stylesheet></noscript><body><header><nav><div><big><a title="NoSQLBench Documentation" href=https://docs.nosqlbench.io></a></big></div><div><div><ul><li><a href=https://docs.nosqlbench.io/tutorials/> Tutorials </a><li><a href=https://docs.nosqlbench.io/guides/> Guides </a><li><a href=https://docs.nosqlbench.io/reference/> Reference </a><li><a href=https://docs.nosqlbench.io/explanations/> Explanations </a><li><a href=https://docs.nosqlbench.io/development/> Development </a><li><a href=https://github.com/nosqlbench/nosqlbench target=_blank> GitHub </a><li><i class="js svgs adjust" id=mode type=reset></i></ul></div><div><div><form autocomplete=off class=js id=searchbox name=goSearch><div class=searchd><input id=searchinput placeholder=Search title=Search><button class="svgs svgm search" title=Search></button></div><div class=results><div id=suggestions></div></div></form></div></div></div></nav></header><main><article><h1><a href=https://docs.nosqlbench.io/reference/bindings/funcref-experimental/>DNN_angular1_neighbors</a></h1><h2 id=dnn-angular1-neighbors>DNN_angular1_neighbors</h2><p>Compute the indices of the neighbors of a given v using DNN mapping. To avoid ambiguity on equidistant neighbors, odd neighborhood sizes are preferred.<ul><li><code>int -> DNN_angular1_neighbors(int: k, int: N, int: modulus) -> int[]</code> <ul><li><em>notes:</em> @param k The size of neighborhood @param N The number of total vectors, necessary for boundary conditions of defined vector @param modulus The modulus used during training of angular1 data; this corresponds to how periodically we cycle back to vectors with the same angle (hence have angular distance zero between them)</ul></ul><h2 id=dnn-euclidean-neighbors>DNN_euclidean_neighbors</h2><p>Compute the indices of the neighbors of a given v using DNN mapping. To avoid ambiguity on equidistant neighbors, odd neighborhood sizes are preferred.<ul><li><code>int -> DNN_euclidean_neighbors(int: k, int: N, int: D) -> int[]</code> <ul><li><em>notes:</em> @param k The size of neighborhood @param N The number of total vectors, necessary for boundary conditions of defined vector @param D Number of dimensions in each vector</ul></ul><h2 id=dnn-euclidean-v>DNN_euclidean_v</h2><ul><li><p><code>long -> DNN_euclidean_v(int: D, long: N) -> float[]</code></p><li><p><code>long -> DNN_euclidean_v(int: D, long: N, double: scale) -> float[]</code></p></ul><h2 id=dnn-euclidean-v-series>DNN_euclidean_v_series</h2><ul><li><code>long -> DNN_euclidean_v_series(int: dimensions, long: population, int: k) -> float[][]</code></ul><h2 id=dnn-euclidean-v-wrap>DNN_euclidean_v_wrap</h2><p>This represents an enumerated population of vectors of some dimension, where any ordinal values which address outside of that enumeration simply wrap with modulo.<ul><li><p><code>long -> DNN_euclidean_v_wrap(int: D, long: N, double: scale) -> float[]</code></p><li><p><code>long -> DNN_euclidean_v_wrap(int: D, long: N) -> float[]</code></p></ul><h2 id=dnnangular1v>DnnAngular1V</h2><ul><li><code>long -> DnnAngular1V(int: D, long: N, long: M) -> float[]</code> <ul><li><em>notes:</em> @param D Dimensions in each vector @param N The number of vectors in the training set @param M The modulo which is used to construct equivalence classes</ul></ul><h2 id=doublearraycache>DoubleArrayCache</h2><p>Precompute the interior double[] values to use as a LUT.<ul><li><code>long -> DoubleArrayCache(io.nosqlbench.virtdata.lib.vectors.primitive.VectorSequence: function) -> double[]</code></ul><h2 id=doublecache>DoubleCache</h2><p>Precompute the interior double[] values to use as a LUT.<ul><li><code>long -> DoubleCache(io.nosqlbench.virtdata.lib.vectors.primitive.DoubleSequence: sequence) -> double</code></ul><h2 id=doublevectorpadleft>DoubleVectorPadLeft</h2><p>Prefix the incoming array with an empty double[] so that it is sized up to at least the given size. If it is already at least that size, pass it through as-is.<ul><li><code>double[] -> DoubleVectorPadLeft(int: size) -> double[]</code></ul><h2 id=doublevectorpadright>DoubleVectorPadRight</h2><p>Suffix the incoming array with an empty double[] so that it is sized up to at least the given size. If it is already at least that size, pass it through as-is.<ul><li><code>double[] -> DoubleVectorPadRight(int: size) -> double[]</code></ul><h2 id=doublevectorprefix>DoubleVectorPrefix</h2><p>Prefix the incoming array with an empty double[] of the given size.<ul><li><code>double[] -> DoubleVectorPrefix(int: size) -> double[]</code></ul><h2 id=doublevectorsuffix>DoubleVectorSuffix</h2><p>Suffix the incoming array with an empty double[] of the given size.<ul><li><code>double[] -> DoubleVectorSuffix(int: size) -> double[]</code></ul><h2 id=doublevectors>DoubleVectors</h2><p>This is a version of the NoSQLBench {@link io.nosqlbench.virtdata.library.basics.shared.util.Combiner} which is especially suited to constructing unique sequences of doubles. This can be to create arbitrarily long vectors in double[] form, where each vector corresponds to a specific character encoding. Based on the maximum cardinality of symbol values in each position, a step function on the unit interval is created for you and used as a source of magnitudes.<p>For example, with a combiner spec of “{@code a-yA-Y*1024}”, the “{@code }a-yA-Y” part creates a character set mapping for 50 distinct indexed character values with the letter acting as a code, and then the “{@code *1024}” repeats ths mapping over 1024 <em>digits</em> of values, which are then concatenated into an array of values as a uniquely encoded vector. In actuality, the internal model is computed separately from the character encoding, so is efficient, although the character encoding can be used to uniquely identify each vector.<p>Note that as with other combiner forms, you can specify a different cardinality for each position, although the automatically computed step function for unit-interval will be based on the largest cardinality. It is not computed separately for each position. Thus, a specifier like “{@code a-z*5;0-9*2}” will only see the last two positions using a fraction of the possible magnitudes, as the a-z element has the most steps at 26 between 0.0 and 1.0.<ul><li><code>long -> DoubleVectors(String: spec) -> double[]</code> <ul><li><p><em>notes:</em> Create a radix-mapped vector function based on a spec of character ranges and combinations. @param spec - The string specifier for a symbolic cardinality and symbol model that represents the vector values</p><li><p><em>example:</em> <code>DoubleVector('0-9*12')</code></p><li><p><em>Create a sequence of vectors encoding a 10-valued step function over 12 dimensions</em></p><li><p><em>example:</em> <code>DoubleVector('01*1024')</code></p><li><p><em>Create a sequence of vectors encoding a 2-valued step function over 1024 dimensions</em></p><li><p><em>example:</em> <code>DoubleVector('a-yA-Y0-9!@#$%^&*()*512')</code></p><li><p><em>Create a sequence of vectors encoding a 70-valued step function over 512 dimensions</em></p></ul></ul><h2 id=floatvectorpadleft>FloatVectorPadLeft</h2><p>Prefix the incoming array with an empty float[] so that it is sized up to at least the given size. If it is already at least that size, pass it through as-is.<ul><li><code>float[] -> FloatVectorPadLeft(int: size) -> float[]</code></ul><h2 id=floatvectorpadright>FloatVectorPadRight</h2><p>Suffix the incoming array with an empty float[] so that it is sized up to at least the given size. If it is already at least that size, pass it through as-is.<ul><li><code>float[] -> FloatVectorPadRight(int: size) -> float[]</code></ul><h2 id=floatvectorprefix>FloatVectorPrefix</h2><p>Prefix the incoming array with an empty float[] of the given size.<ul><li><code>float[] -> FloatVectorPrefix(int: size) -> float[]</code></ul><h2 id=floatvectorsuffix>FloatVectorSuffix</h2><p>Suffix the incoming array with an empty double[] of the given size.<ul><li><code>float[] -> FloatVectorSuffix(int: size) -> float[]</code></ul><h2 id=hasheddoublevectors>HashedDoubleVectors</h2><p>Construct an arbitrarily large vector with hashes. The initial value is assumed to be non-hashed, and is thus hashed on input to ensure that inputs are non-contiguous. Once the starting value is hashed, the sequence of long values is walked and each value added to the vector is hashed from the values in that sequence.<ul><li><p><code>long -> HashedDoubleVectors(Object: sizer, Object: valueFunc) -> double[]</code></p> <ul><li><em>notes:</em> Build a double[] generator with a given size value or size function, and the given long->double function. @param sizer Either a numeric type which sets a fixed dimension, or a long->int function to derive it uniquely for each input @param valueFunc A long->double function</ul><li><p><code>long -> HashedDoubleVectors(Object: sizer, double: min, double: max) -> double[]</code></p><li><p><code>long -> HashedDoubleVectors(Object: sizer) -> double[]</code></p></ul><h2 id=hashedfloatvectors>HashedFloatVectors</h2><p>Construct an arbitrarily large float vector with hashes. The initial value is assumed to be non-hashed, and is thus hashed on input to ensure that inputs are non-contiguous. Once the starting value is hashed, the sequence of long values is walked and each value added to the vector is hashed from the values in that sequence.<ul><li><p><code>long -> HashedFloatVectors(Object: sizer, Object: valueFunc) -> float[]</code></p> <ul><li><em>notes:</em> Build a double[] generator with a given size value or size function, and the given long->double function. @param sizer Either a numeric type which sets a fixed dimension, or a long->int function to derive it uniquely for each input @param valueFunc A long->double function</ul><li><p><code>long -> HashedFloatVectors(Object: sizer, double: min, double: max) -> float[]</code></p><li><p><code>long -> HashedFloatVectors(Object: sizer) -> float[]</code></p></ul><h2 id=hdfbintocql>HdfBinToCql</h2><p>Binding function that accepts a long input value for the cycle and returns a string consisting of the CQL predicate parsed from a single record in an HDF5 dataset<ul><li><code>long -> HdfBinToCql(String: filename, String: predicateName, String[]...: schema) -> String</code></ul><h2 id=hdfdatasettocqlpredicates>HdfDatasetToCqlPredicates</h2><p>Binding function that accepts a long input value for the cycle and returns a string consisting of the CQL predicate parsed from a single record in an HDF5 dataset<ul><li><p><code>long -> HdfDatasetToCqlPredicates(String: filename, String: datasetname, String: parsername) -> String</code></p> <ul><li><em>notes:</em> Create a new binding function that accepts a long input value for the cycle and returns a string @param filename @param datasetname @param parsername</ul><li><p><code>long -> HdfDatasetToCqlPredicates(String: filename, String: datasetname) -> String</code></p></ul><h2 id=hdfdatasettostring>HdfDatasetToString</h2><p>This function reads a vector dataset from an HDF5 file. The entire dataset is parsed into a single String Object with the discreet values separated by the user supplied separator character. It is intended for use only with small datasets where the entire dataset can be read into memory and there is no need to read individual vectors from the dataset. The lambda function simply returns the String representation of the dataset.<ul><li><p><code>long -> HdfDatasetToString(String: filename, String: dataset, String: separator) -> String</code></p> <ul><li><em>notes:</em> Create a new binding function that accepts a long input value for the cycle and returns a string representation of the specified dataset @param filename @param dataset @param separator</ul><li><p><code>long -> HdfDatasetToString(String: filename, String: dataset) -> String</code></p></ul><h2 id=hdfdatasettostrings>HdfDatasetToStrings</h2><p>This function reads a dataset of any supported type from an HDF5 file. The dataset itself is not read into memory, only the metadata (the “dataset” Java Object). The lambda function reads a single vector from the dataset, based on the long input value.<ul><li><code>long -> HdfDatasetToStrings(String: filename, String: datasetName) -> String</code></ul><h2 id=hdfdatasetstostring>HdfDatasetsToString</h2><ul><li><code>long -> HdfDatasetsToString(String: filename, String: DSNameLeft, String: DSNameRight, String: intraSeparator, String: interSeparator) -> String</code></ul><h2 id=hdffiletofloatarray>HdfFileToFloatArray</h2><p>This function reads a vector dataset from an HDF5 file. The dataset itself is not read into memory, only the metadata (the “dataset” Java Object). The lambda function reads a single vector from the dataset, based on the long input value. As currently written this class will only work for datasets with 2 dimensions where the 1st dimension specifies the number of vectors and the 2nd dimension specifies the number of elements in each vector. Only datatypes short, int, and float are supported at this time.<p>This implementation is specific to returning an array of floats<ul><li><code>long -> HdfFileToFloatArray(String: filename, String: datasetName) -> float[]</code></ul><h2 id=hdffiletofloatlist>HdfFileToFloatList</h2><p>This function reads a vector dataset from an HDF5 file. The dataset itself is not read into memory, only the metadata (the “dataset” Java Object). The lambda function reads a single vector from the dataset, based on the long input value. As currently written this class will only work for datasets with 2 dimensions where the 1st dimension specifies the number of vectors and the 2nd dimension specifies the number of elements in each vector. Only datatypes short, int, and float are supported at this time.<p>This implementation is specific to returning a List of Floats, so as to work with the normalization functions e.g. NormalizeListVector and its variants.<ul><li><code>long -> HdfFileToFloatList(String: filename, String: datasetName) -> List&LTFloat></code></ul><h2 id=hdffiletoint>HdfFileToInt</h2><p>This function reads a vector dataset from an HDF5 file. The dataset itself is not read into memory, only the metadata (the “dataset” Java Object). The lambda function reads a single vector from the dataset, based on the long input value. As currently written this class will only work for datasets with 2 dimensions where the 1st dimension specifies the number of vectors and the 2nd dimension specifies the number of elements in each vector. Only datatypes short, int, and float are supported at this time.<p>This implementation is specific to returning a single int<ul><li><code>long -> HdfFileToInt(String: filename, String: datasetName) -> Integer</code></ul><h2 id=hdffiletointarray>HdfFileToIntArray</h2><p>This function reads a vector dataset from an HDF5 file. The dataset itself is not read into memory, only the metadata (the “dataset” Java Object). The lambda function reads a single vector from the dataset, based on the long input value. As currently written this class will only work for datasets with 2 dimensions where the 1st dimension specifies the number of vectors and the 2nd dimension specifies the number of elements in each vector. Only datatypes short, int, and float are supported at this time.<p>This implementation is specific to returning an array of ints<ul><li><code>long -> HdfFileToIntArray(String: filename, String: datasetName) -> int[]</code></ul><h2 id=hdffiletointlist>HdfFileToIntList</h2><p>This function reads a vector dataset from an HDF5 file. The dataset itself is not read into memory, only the metadata (the “dataset” Java Object). The lambda function reads a single vector from the dataset, based on the long input value. As currently written this class will only work for datasets with 2 dimensions where the 1st dimension specifies the number of vectors and the 2nd dimension specifies the number of elements in each vector. Only datatypes short, int, and float are supported at this time.<p>This implementation is specific to returning a List of Integers<ul><li><code>long -> HdfFileToIntList(String: filename, String: datasetName) -> List&LTInteger></code></ul><h2 id=hdffiletolongarray>HdfFileToLongArray</h2><p>This function reads a vector dataset from an HDF5 file. The dataset itself is not read into memory, only the metadata (the “dataset” Java Object). The lambda function reads a single vector from the dataset, based on the long input value. As currently written this class will only work for datasets with 2 dimensions where the 1st dimension specifies the number of vectors and the 2nd dimension specifies the number of elements in each vector. Only datatypes short, int, and float are supported at this time.<p>This implementation is specific to returning an array of longs<ul><li><code>long -> HdfFileToLongArray(String: filename, String: datasetName) -> long[]</code></ul><h2 id=hdffiletolonglist>HdfFileToLongList</h2><p>This function reads a vector dataset from an HDF5 file. The dataset itself is not read into memory, only the metadata (the “dataset” Java Object). The lambda function reads a single vector from the dataset, based on the long input value. As currently written this class will only work for datasets with 2 dimensions where the 1st dimension specifies the number of vectors and the 2nd dimension specifies the number of elements in each vector. Only datatypes short, int, and float are supported at this time.<p>This implementation is specific to returning a List of Longs<ul><li><code>long -> HdfFileToLongList(String: filename, String: datasetName) -> List&LTLong></code></ul><h2 id=hdffiletovarlengthintarray>HdfFileToVarLengthIntArray</h2><p>This function reads a vector dataset from an HDF5 file. The dataset itself is not read into memory, only the metadata (the “dataset” Java Object). The lambda function reads a single vector from the dataset, based on the long input value. As currently written this class will only work for datasets with 2 dimensions where the 1st dimension specifies the number of vectors and the 2nd dimension specifies the number of elements in each vector. Only datatypes short, int, and float are supported at this time.<p>This implementation is specific to returning an array of ints from a dataset that contains variable length arrays of ints.<ul><li><code>long -> HdfFileToVarLengthIntArray(String: filename, String: datasetName) -> int[]</code></ul><h2 id=hdffiletovarlengthintlist>HdfFileToVarLengthIntList</h2><p>This function reads a vector dataset from an HDF5 file. The dataset itself is not read into memory, only the metadata (the “dataset” Java Object). The lambda function reads a single vector from the dataset, based on the long input value. As currently written this class will only work for datasets with 2 dimensions where the 1st dimension specifies the number of vectors and the 2nd dimension specifies the number of elements in each vector. Only datatypes short, int, and float are supported at this time.<p>This implementation is specific to returning a List of type Integer from a dataset that contains variable length arrays of ints.<ul><li><code>long -> HdfFileToVarLengthIntList(String: filename, String: datasetName) -> List&LTInteger></code></ul><h2 id=hdfpredicatestocql>HdfPredicatesToCql</h2><p>Binding function that accepts a long input value for the cycle and returns a string consisting of the CQL predicate parsed from a single record in an HDF5 dataset<ul><li><code>long -> HdfPredicatesToCql(String: filename, String: datasetName, String: serDesType) -> String</code> <ul><li><em>notes:</em> Create a new binding function that accepts a long input value for the cycle and returns a string @param filename The HDF5 file to read the predicate dataset from @param datasetName The name of the dataset internal to the HDF5 file @param serDesType The type of serialization/deserialization to use for the predicate</ul></ul><h2 id=intarraytostring>IntArrayToString</h2><ul><li><code>int[] -> IntArrayToString() -> String</code></ul><h2 id=normalizecqlvector>NormalizeCqlVector</h2><p>Normalize a vector in List form, calling the appropriate conversion function depending on the component (Class) type of the incoming List values.<ul><li><code>com.datastax.oss.driver.api.core.data.CqlVector&LTN> -> NormalizeCqlVector() -> com.datastax.oss.driver.api.core.data.CqlVector</code></ul><h2 id=normalizedoublelistvector>NormalizeDoubleListVector</h2><p>Normalize a vector.<ul><li><code>List&LTDouble> -> NormalizeDoubleListVector() -> List&LTDouble></code></ul><h2 id=normalizedoublevector>NormalizeDoubleVector</h2><ul><li><code>double[] -> NormalizeDoubleVector() -> double[]</code></ul><h2 id=normalizefloatlistvector>NormalizeFloatListVector</h2><p>Normalize a vector.<ul><li><code>List&LTFloat> -> NormalizeFloatListVector() -> List&LTFloat></code></ul><h2 id=normalizefloatvector>NormalizeFloatVector</h2><ul><li><code>float[] -> NormalizeFloatVector() -> float[]</code></ul><h2 id=normalizelistvector>NormalizeListVector</h2><p>Normalize a vector in List form, calling the appropriate conversion function depending on the component (Class) type of the incoming List values.<ul><li><code>List -> NormalizeListVector() -> List</code></ul><h2 id=repeatlist>RepeatList</h2><p>Repeat the incoming list into a new list, filling it to the given size.<ul><li><code>List -> RepeatList(int: size) -> List</code> <ul><li><p><em>notes:</em> Create a list repeater to build up a list from a smaller list. @param size - the total size of the new list</p><li><p><em>example:</em> <code>RepeatList(50)</code></p><li><p><em>repeat the incoming values into a new List of size 50</em></p></ul></ul><h2 id=sequenceof>SequenceOf</h2><p>SequenceOf bindings allow you to specify an order and count of a set of values which will then be repeated in that order. SequenceOf bindings allow you to specify an order and count of a set of values which will then be repeated in that order.<ul><li><code>long -> SequenceOf(int: ignored, String: spec) -> int</code> <ul><li><em>notes:</em> <p>This function produces values from a lookup table for direct control of numerical sequences. The sequence spec is a string containing the sequence values and their occurences, defaulting to 1 each. Example: “1:6 2 3 4 5”, which means “1 at a relative frequency of 6 and 2, 3, 4, and 5 at a relative frequency of 1 each. This will yield pattern “1, 1, 1, 1, 1, 1, 2, 3, 4, 5, 1, 1, 1, 1, 1, 1, 2, 3, 4, 5, …”</ul></ul><p>Each implementation of {@link SequenceOf} must include a type sigil as the first parameter to disambiguate it from the others.<p>@param ignored any long value, discarded after signature matching. The exampleValue is thrown away, but is necessary for matching the right version of SequenceOf. @param spec A string of numbers separated by spaces, semicolons, or commas. This is the sequence spec..<ul><li><p><em>example:</em> <code>SequenceOf(1L,'3:3 2:2 1:1')</code></p><li><p><em>Generate sequence 3,3,3,2,2,1</em></p><li><p><em>example:</em> <code>SequenceOf(1L,'1000:99 1000000:1')</code></p><li><p><em>Generate sequence 1000 (99 times) and then 1000000 (1 time)</em></p><li><p><code>long -> SequenceOf(long: ignored, String: spec) -> long</code></p> <ul><li><em>notes:</em> <p>This function produces values from a lookup table for direct control of numerical sequences. The sequence spec is a string containing the sequence values and their occurences, defaulting to 1 each. Example: “1:6 2 3 4 5”, which means “1 at a relative frequency of 6 and 2, 3, 4, and 5 at a relative frequency of 1 each. This will yield pattern “1, 1, 1, 1, 1, 1, 2, 3, 4, 5, 1, 1, 1, 1, 1, 1, 2, 3, 4, 5, …”</ul></ul><p>Each implementation of {@link SequenceOf} must include a type sigil as the first parameter to disambiguate it from the others.<p>@param ignored any long value, discarded after signature matching. The exampleValue is thrown away, but is necessary for matching the right version of SequenceOf. @param spec A string of numbers separated by spaces, semicolons, or commas. This is the sequence spec..<ul><li><em>example:</em> <code>SequenceOf(1L,'3:3 2:2 1:1')</code><li><em>Generate sequence 3,3,3,2,2,1</em><li><em>example:</em> <code>SequenceOf(1L,'1000:99 1000000:1')</code><li><em>Generate sequence 1000 (99 times) and then 1000000 (1 time)</em></ul><h2 id=tocqlvector>ToCqlVector</h2><ul><li><p><code>double[] -> ToCqlVector() -> com.datastax.oss.driver.api.core.data.CqlVector</code></p><li><p><code>float[] -> ToCqlVector() -> com.datastax.oss.driver.api.core.data.CqlVector</code></p><li><p><code>List -> ToCqlVector() -> com.datastax.oss.driver.api.core.data.CqlVector</code></p></ul><h2 id=tofloatvector>ToFloatVector</h2><ul><li><code>double[] -> ToFloatVector() -> float[]</code></ul><h2 id=tokenmapfilecycle>TokenMapFileCycle</h2><p>Utility function used for advanced data generation experiments.<ul><li><code>int -> TokenMapFileCycle(String: filename, boolean: loopdata, boolean: ascending) -> long</code></ul><h2 id=tokenmapfilenextcycle>TokenMapFileNextCycle</h2><p>Utility function used for advanced data generation experiments.<ul><li><code>int -> TokenMapFileNextCycle(String: filename, boolean: loopdata, boolean: ascending) -> long</code></ul><h2 id=tokenmapfilenexttoken>TokenMapFileNextToken</h2><p>Utility function used for advanced data generation experiments.<ul><li><code>int -> TokenMapFileNextToken(String: filename, boolean: loopdata, boolean: ascending) -> long</code></ul><h2 id=tokenmapfiletoken>TokenMapFileToken</h2><p>Utility function used for advanced data generation experiments.<ul><li><code>int -> TokenMapFileToken(String: filename, boolean: loopdata, boolean: ascending) -> long</code></ul><h2 id=triangularstep>TriangularStep</h2><p>Compute a value which increases monotonically with respect to the cycle value. All values for f(X+(m>=0)) will be equal or greater than f(X). In effect, this means that with a sequence of monotonic inputs, the results will be monotonic as well as clustered. The values will approximate input/average, but will vary in frequency around a simple binomial distribution.<p>The practical effect of this is to be able to compute a sequence of values over inputs which can act as foreign keys, but which are effectively ordered.<h3 id=call-for-ideas>Call for Ideas</h3><p>Due to the complexity of generalizing this as a pure function over other distributions, this is the only function of this type for now. If you are interested in this problem domain and have some suggestions for how to extend it to other distributions, please join the project or let us know.<ul><li><p><code>long -> TriangularStep(long: average, long: variance) -> long</code></p> <ul><li><em>example:</em> <code>TriangularStep(100,20)</code><li><em>Create a sequence of values where the average is 100, but the range of values is between 80 and 120.</em><li><em>example:</em> <code>TriangularStep(80,10)</code><li><em>Create a sequence of values where the average is 80, but the range of values is between 70 and 90.</em></ul><li><p><code>long -> TriangularStep(long: average) -> long</code></p></ul><h2 id=uniformvectorsizedstepped>UniformVectorSizedStepped</h2><p>Create a vector which consists of a number of uniform vector ranges. Each range is set as [min,max] inclusive by a pair of double values such as 3.0d, 5.0d, … You may provide an initial integer to set the number of components in the vector. After the initial (optional) size integer, you may provide odd, even pairs of min, max. If a range is not specified for a component which is expected from the size, the it is automatically replaced with a unit interval double variate.<ul><li><code>long -> UniformVectorSizedStepped(Number[]...: dims) -> List&LTDouble></code> <ul><li><em>example:</em> <code>UniformVectorSizedStepped(3)</code><li><em>create a 3-component vector from unit interval variates</em><li><em>example:</em> <code>UniformVectorSizedStepped(1.0d,100.0d,5.0d,6.0d)</code><li><em>create a 2-component vector from the specified uniform ranges [1.0d,100.0d] and [5.0d,6.0d]</em><li><em>example:</em> <code>UniformVectorSizedStepped(2,3.0d,6.0d)</code><li><em>create a 2-component vector from ranges [3.0d,6.0d] and [0.0d,1.0d]</em></ul></ul><nav><div><a href=https://docs.nosqlbench.io/reference/bindings/funcref-vectors/>‹ BaseVectors</a></div><div><a href=https://docs.nosqlbench.io/reference/bindings/funcref-objects/> Distance ›</a></div></nav></article></main><footer><div class=c><nav class=tpad><div></div></nav><p>© <span id=year>2025</span> NoSQLBench Documentation<p>Powered by <a href=https://www.getzola.org/ target=_blank>Zola</a> & <a href=https://github.com/jieiku/abridge/ target=_blank>Abridge</a></div></footer><span class=topout> <span class=topleft> </span><a title="Back to Top" class=top href=#><i class="svgs svgh angu"></i></a> </span>