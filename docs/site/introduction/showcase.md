---
title: Showcase
description: What makes NoSQLBench unique, starting with the virtual dataset.
audience: user
diataxis: howto
tags:
- site
- docs
component: site
topic: docops
status: live
owner: '@nosqlbench/docs'
generated: false
weight: 3
---

NoSQLBench has enjoyed a history of unique innovationâ€”driven by the vision of its users and
builders, forged by the need for practical methods to test modern systems. This section covers a
sampling of what makes NoSQLBench unique. Many of these features simply could not be found in other
testing systems when they were needed, so NoSQLBench took form as we solved them one after another
in the same tool space. The result is a powerful runtime and system of components that can be
adapted to a variety of testing needs.

## Virtual Data Set

The _Virtual Dataset_ capabilities within NoSQLBench allow you to generate data on the fly. There
are many reasons for using this technique in testing, but it is often overlooked or taken for
granted.

This has multiple positive effects on the fidelity of a test:

- It is far more efficient than interacting with storage systems and piping data around. In most
  cases, even loading data from lightweight storage like NVMe is more time intensive than simply
  generating it.
- As such, it leaves significant headroom for introducing other valuable capabilities into the test
  system, like advanced rate metering and coordinated omission awareness.
- Changing the generated data is as easy as changing the recipe.
- The efficiency of the client is often high enough to support single-client test setups without an
  appreciable loss of capacity.
- Because of modern procedural generation techniques, the variety and shape of data available is
  significant. Increasing the space of possibilities is a matter of adding new algorithmsâ€”there is
  no data bulk to manage.
- Sophisticated test setups that are highly data dependent are portable. All you need is the test
  client. The building blocks for data generation are included, and many pre-built testing scenarios
  are already wired to use them.
- It is straightforward to design incremental data generation schemes which produce monotonic
  identifiers, pseudo-random traversal over the values, or statistically shaped versions of
  incremental or pseudo-random values.

Additional details of this approach are explained below.

##### Industrial Strength

The algorithms used to generate data are based on advanced techniques in the realm of variate
sampling. We go to great lengths to ensure that data generation is efficient and as close to O(1) per
sample as possible.

One technique used to achieve this is to initialize and cache data in high-resolution lookup tables
for distributions which may otherwise perform differently depending on their density functions. The
existing Apache Commons Math libraries have been adapted into a set of interpolated inverse
cumulative distribution sampling functions. This means you can use them just as you would a uniform
distribution, and once initialized, they sample with identical overhead. By changing your test
definition you don't accidentally change the behavior of your test clientâ€”only the data, as
intended.

##### A Purpose-Built Tool

Many other testing systems avoid building a dataset generation component. It's a tough problem to
solve, so it's often avoided. Instead, they use libraries like Faker or other sources of data that
weren't designed for testing at scale. Faker is well named: it was meant as a vignette and
wire-framing library, not a source of test data for realistic results. If you rely on a faker variant
for scale testing, you will almost certainly get invalid results that do not represent how a system
would perform in production.

The virtual dataset component of NoSQLBench is designed for high scale and realistic data streams. It
uses the limits of the JVM data types to simulate high-cardinality datasets that approximate
production data distributions for realistic and reproducible results.

##### Deterministic

The data generated by the virtual dataset libraries is deterministic. For a given cycle in a test,
the synthesized operation will be the same from one session to the next. This is intentional. If you
want to perturb the test data from one session to the next, simply select a different set of cycles
as your basis.

This means that if you find something interesting in a test run, you can go back to it just by
specifying the cycles in question. It also means that you aren't losing comparative value between
tests with additional randomness thrown in. The data you generate will still look random to the
human eye, but that doesn't mean that it won't be reproducible.

##### Statistically Shaped

If you want a normal distribution, you can have it simply by specifying `Normal(50,10)`. The values
drawn from this sampling function are deterministic *and* normal. If you want another distribution,
you can have it. All the distributions provided by the Apache Commons Math libraries are supported.
You can ask for a stream of floating-point values a trillion values long, in any order. You can use
discrete or continuous distributions with whatever parameters you need.

##### Best of Both Worlds

Some worry that fully synthetic testing data is not realistic enough. The procedural data generation
approach allows you to have the benefits of testing agility from low-entropy testing tools while
retaining nearly all the benefits of real testing data.

For example, using the alias sampling method and a published US census (public domain) list of names
and surnames that occurred more than 100 times, we can provide accurate samples of names according to
the published labels and weights. The alias method allows us to sample accurately in *O(1)* time
from the entire dataset by turning a large number of weights into two uniform samples. You will
simply not find a better way to sample realistic (US) names than this. Any dataset that you have in
CSV form with a weight column can also be used this way, so you're not strictly limited to US census
data.

##### Java Idiomatic Extension

The virtual dataset functions are available as idiomatic Java classes and can be combined in
flexible ways. Advanced users can compose new generators, wrap existing ones, and plug them directly
into workload definitions without having to manage external fixture data.

ðŸ‘‰ Some features discussed here are only for advanced testing scenarios. First-time users should
become familiar with the basic options first.

##### Hybrid Rate Limiting

Rate limiting is a complicated endeavor if you want to do it well. The basic rub is that going fast
means you have to be less accurate, and vice versa. As such, rate limiting is a parasitic drain on
any system. The act of rate limiting itself poses a limit to the maximum rate, regardless of the
settings you pick.

The rate limiter in NoSQLBench provides performance and accuracy comparable to other Java libraries,
but it *also* has advanced features:

- A sliding scale between average rate limiting and strict rate limiting (bursting).
- Internal accumulation of delay time for coordinated-omission-friendly metrics that are tracked per
  operation.
- Runtime reconfiguration, including burst rate.
- Metrics that capture both performance data and the configured limiter values.
- Advanced scripting helpers which allow you to read data directly from histogram reservoirs or
  control the reservoir window programmatically.

##### Flexible Error Handling

Error handling within an activity is highly configurable. For example, with the CQL activity type
you can route handling for any of the known exception types: count errors, log them, or automatically
retry operations up to a configurable number of attempts. This lets you decide what your test is
aboutâ€”intentional over-saturation, stability testing, or anything in between.

##### Cycle Logging

It is possible to record the result status of each and every cycle in a NoSQLBench test run. If the
results are mostly homogeneous, the run-length encoding (RLE) of the results reduces the output file
to a fraction of the number of cycles. Errors are mapped to ordinals by error type, and these
ordinals are stored in a direct RLE-encoded log file. You can also convert the cycle log into textual
form for post-processing and vice versa.

##### Op Sequencing

Operations in NoSQLBench are planned for execution based on stable ordering that you can configure.
Statement forms are mixed according to their relative ratios. The currently supported schemes are
round robin with exhaustion (bucket), duplicate-in-order (concat), and spreading each statement out
over the unit interval (interval). These cover most configuration scenarios without requiring users
to micromanage statement templates.
